{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing and downloading neccesary libraries\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.utils import Bunch\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import WordNetLemmatizer\n",
    "import numpy as np\n",
    "from github import Github\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_words(data):   # FOR PROCESSING DOC (stop word)\n",
    "  \n",
    "    processed_data = []\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_splitter = RegexpTokenizer(r'\\w+')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    processing = word_splitter.tokenize(data)\n",
    "    for i in processing:\n",
    "        i = i.lower()\n",
    "        if i not in stop_words and i.isalpha() and len(i)>2:\n",
    "            processed_data.append(i)\n",
    "    for j in processed_data:\n",
    "        lemmatizer.lemmatize(j)\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "def get_unique(data):   # GET UNQUIE WORDS\n",
    "    unique_list = []\n",
    "    for word in data:\n",
    "        if word in unique_list:\n",
    "            \"do nothing\"\n",
    "        else:\n",
    "            unique_list.append(word)\n",
    "    return(unique_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env github_token = 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   # picking up an enviromental variable that doesnt get saved to repo when pushed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CTC_COMMONWORDS DICTIONARY UPDATE\n",
    "\n",
    "token = os.getenv('github_token')\n",
    "\n",
    "g = Github(token)\n",
    "\n",
    "my_repos = g.get_organization(\"codethecity\").get_repos()\n",
    "wordsfound = []\n",
    "for repo in my_repos:\n",
    "    try:\n",
    "        readme = repo.get_readme().decoded_content\n",
    "        cleaned_readme = process_words(str(readme))\n",
    "        wordsfound+=get_unique(cleaned_readme)\n",
    "    except Exception as e:\n",
    "        print(e,\", no readme!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECTING  TOP 15 MOST COMMON WORDS TO FILTER NEXT\n",
    "percentage=15     \n",
    "ctc_words = Counter(wordsfound)\n",
    "x= round(percentage/100*len(ctc_words))\n",
    "ctc_commonwords = ctc_words.most_common(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RE PROCESSING WORDS REMOVING COMMON CTC WORDS FUNCTION\n",
    "\n",
    "\n",
    "def process_words_ctc(data: str):\n",
    "  \n",
    "    processed_data = []\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_splitter = RegexpTokenizer(r'\\w+')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    processing = word_splitter.tokenize(data)\n",
    "    for i in processing:\n",
    "        i = i.lower()\n",
    "        if i not in stop_words and i.isalpha() and (len(i)>2) and (i not in [word[0] for word in ctc_commonwords]) and (i not in manual_filter):\n",
    "            processed_data.append(i)\n",
    "    for j in processed_data:\n",
    "        lemmatizer.lemmatize(j)\n",
    "\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT REPONAME: TAG PAIRS\n",
    "\n",
    "manual_filter = ['codethecity','ncodethecity','div', 'class', 'col','npercentage', 'mailto']\n",
    "\n",
    "token = os.getenv('github_token')\n",
    "\n",
    "g = Github(token)\n",
    "\n",
    "my_repos = g.get_organization(\"codethecity\").get_repos()\n",
    "repo_tag = {}\n",
    "for repo in my_repos:\n",
    "    try:\n",
    "        readme = repo.get_readme().decoded_content\n",
    "        cleaned_readme = process_words_ctc(str(readme)) # ctc stop word are flitered but words are allowed to repeat to give potential tags more relevance\n",
    "        tag = Counter(cleaned_readme+str(repo.name).split()) # using repo name to give potential tags more relevance\n",
    "        top_tag = [item[0] for item in tag.most_common(10)]                  # repo name as key, tags as value\n",
    "        repo_tag[repo.name] = top_tag\n",
    "#        print(repo.name,' : ',top_tag)\n",
    "    except:\n",
    "        tag = Counter(repo.name.split()) # using repo name to give potential tags more relevance\n",
    "        repo_tag[repo.name] = [item[0] for item in tag.most_common(10)]\n",
    "#        print( \"no readme!\",repo.name,' : ',[item for item in tag])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(repo_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repo_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
